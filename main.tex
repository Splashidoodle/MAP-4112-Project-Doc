\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}
\title{MAP 4112 Project}
\author{Marco Osorio, Sparsh Pandey}
\date{November 2025}

\begin{document}

\maketitle

\section{Introduction}

With the vast amounts of data available for models to train on, a challenge emerges: computational resources are often wasted processing high-dimensional data. Even on a relatively small-sized dataset, such as the Labeled Faces in the Wild (LFW) dataset with 5,749 identities spanning 13,233 images, the running time can be visibly slow. Moreover, the extra dimensions provide little to no benefit in performance, sometimes even hindering it. Due to this, the data needs to be preprocessed by performing a dimension reduction. 

This can be fulfilled with Principal Component Analysis (PCA): an unsupervised dimension reduction algorithm that identifies directions with the highest variances. By computing an orthogonal basis ordered from the highest to lowest explained variance, PCA can then take the first \textit{n} directions that add up to the \textit{x}\% variance threshold.

To demonstrate PCA, a dataset rich in dimensionality is best suited to show the differences in runtime and accuracy with and without PCA. Datasets of images and videos fit this criterion. These datasets are often filled with noise and dimensions that contribute little to training. A motivating example would be a video at sixty frames per second. One frame to the next is likely not contributing a large enough difference for training. The LFW dataset works well for this as well, which consists of centered faces of famous people that are labeled. This dataset is small enough that it can be run locally in a reasonable amount of time, while also showcasing the difference with and without PCA. To train on the projected matrix produced by PCA, we will use a Collaborative Representation Classifier (CRC). 

\section{Discussion}
In order to classify faces, PCA will be implemented in order to use the eigenfaces method. Eigenfaces work by projecting our data matrix onto a subspace composed of eigenvectors of the covariance matrix. These eigenvectors, also known as principal components, are orthogonal and capture the directions with the most variance, which will allow our projected data to retain the information that matters most when classifying faces.

A main advantage of PCA is that a large variance ratio can usually be achieved with relatively few principal components. The number of principal components used is k, which is usually smaller than the feature space or the same size, but never larger. This means when we project onto these k components, we usually reduce the dimensions we are working with which reduces complexity and means we only store the features that capture the most variance between faces. A main disadvantage of PCA is that it does not work with nonlinear features. Since our principal components are orthogonal to each other, any pattern or relationships between features that are not linear will not be recognized which results in poor classification of faces.

\section{Implementation}
To demonstrate PCA, the LFW dataset was used. CRC was implemented to test PCA. The implementation was in Python, using \textit{sklearn}. After loading the dataset, only images with greater than 50 images were kept. They were then resized to 40\% of the original resolution and flattened from a 50 * 37 resolution to 1850. Keeping images greater than or equal to 50 images was an important step in ensuring better training data for a more simple training model unlike an FCNN or CNN. A train/val/test split of 60/15/25 was used. The samples were then L2 normalized for CRC to perform better. 

PCA was implemented by three functions. \textit{pca\_fit\_svd} computed the SVD on the samples, converted to eigenvalues, and found the explained variance by dividing by the number of samples - 1. Finally, the total variance was found by adding all explained variances together, and the ratio was computed. \textit{pca\_choose\_k\_from\_ratio} took the first k components generated from the variance ratios until the specified threshold was reached. A third function \textit{pca\_project\_splits} ran the first and second functions to get the top k components, and projected the train, val, and tests to the top \textit{k} components. 

Since the CRC algorithm is out of scope for this project, it will only be briefly discussed. The CRC algorithm was not just called by a built-in python function but implemented. A precompute function solved for P, which is used for predictions. The \textit{crc\_predict\_one} function then made the prediction for a single sample and the \textit{crc\_predict\_set} used the \textit{crc\_predict\_one} function over all samples. 

Finally, two experiments were ran, each with different hyperparameters. The first ran CRC without PCA at different $\lambda$ values. The values used ranged from 0.001 to 10, each scaling by 10. Whichever run generated the highest validation accuracy was used on the test set. This was repeated for CRC with PCA with the addition of a new parameter, ratio. Ratio specifies how much of the variance to keep. The configuration with the highest validation accuracy was used on the test set. All experiments are printed in the Jupyter Notebook provided. 

\section{Results}
After implementation, CRC with PCA was able to achieve a test accuracy
of 50.51\% and CRC without PCA achieved a test accuracy of 48.21\%. For
variance ratios 90\%, 95\%, 99\%, the following respective k values were found,
81, 154, 373. By setting $k = 373$,we went from working in our original feature space of 1850, to just 373 while still maintaining 99\% variance. The runtime to run all configurations of CRC without PCA was 7.6 seconds, while the runtime to run all configurations of CRC WITH PCA was 3.8 seconds. The latter had 15 configurations, while the former just had 5. 
\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{crc_comparison_table.png}
\label{fig:crc_comparison_table}
\end{figure}

Using the information from the comparison table, we see our validation accuracy increases with an increasing number of k components. Similarly, increasing the number of principal components k can only increase the accuracy of the classification or keep it the same, but never decrease it. CRC with PCA will execute faster because it reduces the dimensions of the data we are working on while also retaining as much information on it. CRC by itself will operate on the entire dataset which is considerably slower on larger datasets. 

\begin{figure}[h]
    \centering
    \hspace{-5cm}
    \begin{minipage}{\textwidth}
        \centering
        \includegraphics[width=1.5\linewidth]{confusion_matrices.png}
        \label{fig:confusion matrix}
    \end{minipage}\hfill
\end{figure}

\vspace{2.5 cm}

From our confusion matrices, we see that only 188 of the faces were classified correctly when using CRC without PCA but when combining with PCA and using 373 principal components, we were able to classify 193 faces correctly. This shows how PCA can help filter unnecessary information which will reduce overfitting and even increase accuracy in some cases. Since George W Bush seemed to have a skew in the number of photos, CRC tended to predict him often, hence the outlier column.

\section{Conclusion}
This project showed that PCA is very useful for facial classification. By retaining the most informative variance, we were able to reduce the dimensions we are working with while also improving our classification accuracy. However this does have limitations. Since our principal components are eigenvectors, they are linearly independent from each other. This makes any non-linearly independent patterns that can be useful difficult to identify, making classification harder. It also demonstrates how removing too much variance does make the model less accurate. However, reducing even 1\% of the variance makes a huge difference in runtime and compromises little to no accuracy. 
\end{document}

