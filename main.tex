\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}
\title{MAP 4112 Project}
\author{Marco Osorio, Sparsh Pandey}
\date{November 2025}

\begin{document}

\maketitle

\section{Introduction}

With the vast amounts of data available for models to train on, a challenge emerges: computational resources are often wasted processing high-dimensional data. Even on a relatively small-sized dataset, such as the Labeled Faces in the Wild (LFW) dataset with 5,749 identities spanning 13,233 images, the running time can be visibly slow. Moreover, the extra dimensions provide little to no benefit in performance, sometimes even hindering it. Due to this, the data needs to be preprocessed by performing a dimension reduction. 

This can be fulfilled with Principal Component Analysis (PCA): an unsupervised dimension reduction algorithm that identifies directions with the highest variances. By computing an orthogonal basis ordered from the highest to lowest explained variance, PCA can then take the first \textit{n} directions that add up to the \textit{x}\% variance threshold.

To demonstrate PCA, a dataset rich in dimensionality is best suited to show the differences in runtime and accuracy with and without PCA. Datasets of images and videos fit this criterion. These datasets are often filled with noise and dimensions that contribute little to training. A motivating example would be a video at sixty frames per second. One frame to the next is likely not contributing a large enough difference for training. The LFW dataset works well for this as well, which consists of centered faces of famous people that are labeled. This dataset is small enough that it can be run locally in a reasonable amount of time, while also showcasing the difference with and without PCA. To train on the projected matrix produced by PCA, we will use a Collaborative Representation Classifier (CRC). 

\section{Discussion}
In order to classify faces, Principal Component Analysis (PCA) will be implemented in order to use the eigenfaces method. Eigenfaces work by projecting our data matrix onto a subspace composed of eigenvectors of the covariance matrix. These eigenvectors, also known as principal components, are orthogonal and capture the directions with the most variance, which will allow our projected data to retain the information that matters most when classifying faces.

A main advantage of PCA is that a large variance ratio can usually be achieved with relatively few principal components. The number of principal components used is k, which is usually smaller than the feature space or the same size, but never larger. This means when we project onto these k components, we usually reduce the dimensions we are working with which reduces complexity and means we only store the features capture the most variance between faces. A main disadvantage of PCA is that it does not work with nonlinear features. Since our principal components are orthogonal to each other, any pattern or relationships between features that are not linear will not be recognized which results in poor classification of faces.

In order to apply to a learning application, a Collaborative Representation Classifier (CRC) will be trained on the projected matrix. The dataset of president's faces consists of will be filtered to only includes faces with at least 5 examples. Some preprocessing steps will be taken which include tuning the hyperparmeter k on different variance ratios and including translations of faces to make our classifier more accurate. The performance of PCA using a classifier will involve splitting data into a 80\% training and 20\% test split. Eigenfaces will be computed from the training set and be fitted onto our classifer which will then be using to predict our testing set. 
\end{document}
